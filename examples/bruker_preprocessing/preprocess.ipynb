{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bruker Preprocessing Pipeline for Pilot Girder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-ins\n",
    "from pathlib import *\n",
    "import itertools\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "# VIP\n",
    "from vip_client.utils import vip\n",
    "# Girder\n",
    "from girder_client import GirderClient, HttpError"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**User variables**: should be checked and supplied at each execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API keys\n",
    "VIP_KEY = os.environ[\"VIP_API_KEY\"] # Your VIP API key here\n",
    "GIRDER_KEY = os.environ[\"GIRDER_API_KEY\"] # Your Girder API key here\n",
    "\n",
    "# Parameters to feed in the preprocessing pipeline, in addition to the preprocessing files.\n",
    "PARAMETERS = {\n",
    "    # [Argument]: [value]\n",
    "    \"outname\"   : \"sigproc\",\n",
    "    # Put additional parameters below\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Constant variables**: should not be changed unless the dataset structure or the pipeline have been modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the pipeline performing the preprocessing on VIP\n",
    "PIPELINE_ID = \"Bruker-preproc/0.3\" \n",
    "# Name of the preprocessing files for VIP\n",
    "RAWDATA_FILES = {\n",
    "    # [File name]   : [Argument for `PIPELINE_ID`]\n",
    "    \"acqp\"          : \"acqp\",\n",
    "    \"fid\"           : \"fid\",\n",
    "    \"method\"        : \"method\",\n",
    "    \"rawdata.job0\"  : \"rawjob0\",\n",
    "    \"fid.refscan\"   : \"refscan\" \n",
    "}\n",
    "# Maximum parallel jobs submitted to VIP\n",
    "MAX_VIP_JOBS = 10\n",
    "# Instantiate Girder client with Pilot URL\n",
    "GIRDER_CLIENT = GirderClient(apiUrl='https://pilot-warehouse.creatis.insa-lyon.fr/api/v1')\n",
    "# Prefix to add for launching VIP jobs on this Girder dataset\n",
    "GIRDER_PREFIX = \"pilotGirder:\"\n",
    "# Girder path to the MRS dataset\n",
    "DATASET_PATH = \"/collection/ishc-myeline\"\n",
    "# Name of the target sequence in which the rawdata should be found (for partial, case-insensitive match in the folder names)\n",
    "TARGET_SEQUENCE = \"STEAM\"\n",
    "# Path to the raw data within each sequence folder (must be a Girder folder)\n",
    "RAWDATA_FOLDER = \"Raw\"\n",
    "# Path to the processed data within each sequence folder (must be a Girder folder)\n",
    "OUTPUT_FOLDER = \"Proc_\" + time.strftime(\"%y-%m-%d_%H-%M-%S\", time.localtime())\n",
    "# Metadata for the output folder\n",
    "OUTPUT_METADATA = {\n",
    "    \"pipeline_id\"   : PIPELINE_ID,\n",
    "    \"parameters\"    : PARAMETERS\n",
    "}\n",
    "OUTPUT_DESCRIPTION = \"\"\"\n",
    "Preprocessing with the Crea-PASTIS algorithm on the Virtual Imaging Platform.\n",
    "https://gitlab.in2p3.fr/pilot/rmn/processing/bruker-spectro-processing-pipeline\n",
    "https://vip.creatis.insa-lyon.fr\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedure"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Procedure\n",
    "\n",
    "Search for `RAWDATA_FOLDER`(*Raw*) under the `DATASET_PATH` (*ishc-myeline*) whose parent includes the `TARGET_SEQUENCE` (*STEAM*) in its name.\n",
    "\n",
    "For each new group of `MAX_VIP_JOBS` (*e.g.*, 10) signals found in the dataset : \n",
    "- Launch the preprocessing pipeline on those signals, using all `RAWDATA_FILES` found found each signal ;\n",
    "- While the pipeline is still running, look for the next group of signals to process ;\n",
    "- When the pipeline is over, save the results in the `OUTPUT_FOLDER` (*Proc_[timestamp]*) of each signal.\n",
    "\n",
    "Save the signal paths and processing metadata in a local JSON file (`OUTPUT_FOLDER`.json)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criterion to detect the target rawdata folder\n",
    "def is_rawdata_folder(folder: dict):\n",
    "    return (\n",
    "        (RAWDATA_FOLDER == folder[\"path\"].name) \n",
    "        and (TARGET_SEQUENCE.lower() in folder[\"path\"].parent.name.lower())\n",
    "        # TODO: SUPPRESS BELOW WHEN OPTIONAL FILES WILL BE SUPPORTED BY VIP ------------------------------------\n",
    "        # Check that all required input files are present \n",
    "        and (set(RAWDATA_FILES) < set([item[\"name\"] for item in GIRDER_CLIENT.listItem(folderId=folder[\"id\"])]))\n",
    "        # ------------------------------------------------------------------------------------------------------\n",
    "    )\n",
    "# ------------------------------------------------  \n",
    "\n",
    "# Generator to find each rawdata folder matching the criterion\n",
    "def all_rawdata_folders(node) -> dict:\n",
    "    # Wait to avoid request overflow\n",
    "    time.sleep(0.1)\n",
    "    # Search in all subfolders\n",
    "    for folder in GIRDER_CLIENT.listFolder(parentId=node[\"id\"], parentFolderType=node[\"type\"]):\n",
    "        # Get all the information we need\n",
    "        folder_data = {\n",
    "            \"path\": node[\"path\"] / folder[\"name\"],\n",
    "            \"id\": folder[\"_id\"],\n",
    "            \"parentId\": folder[\"parentId\"],\n",
    "            \"type\": folder[\"_modelType\"]\n",
    "        }\n",
    "        # Check if current folder is Rawdata\n",
    "        if is_rawdata_folder(folder_data):\n",
    "            yield folder_data\n",
    "        else:\n",
    "            # Search the current folder\n",
    "            yield from all_rawdata_folders(folder_data)\n",
    "# ------------------------------------------------  \n",
    "\n",
    "# Function to extract the proprocessing files from an MRS acquisition folder\n",
    "def get_files(folder: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Looks for rawdata files in `folder`.\n",
    "    Returns a dictionnary mapping each file to its path on Girder.\n",
    "    \"\"\"\n",
    "    # Retrieve all items in the file list\n",
    "    return {\n",
    "        # [Argument for the pipeline]: [path of the Girder item]\n",
    "        RAWDATA_FILES[item[\"name\"]]  : GIRDER_PREFIX + file[\"_id\"]\n",
    "            for item in GIRDER_CLIENT.listItem(folderId=folder[\"id\"]) if item[\"name\"] in RAWDATA_FILES\n",
    "            for file in GIRDER_CLIENT.listFile(itemId=item[\"_id\"], limit=1) # only 1 file\n",
    "    }\n",
    "# ------------------------------------------------\n",
    "\n",
    "# Generator to make lists of rawdata files with respect to the maximum number of jobs on VIP\n",
    "def get_data():\n",
    "    # Initiate folder search\n",
    "    dataset = GIRDER_CLIENT.resourceLookup(DATASET_PATH)\n",
    "    iterator = all_rawdata_folders({\n",
    "        \"path\": PurePosixPath(DATASET_PATH),\n",
    "        \"id\": dataset[\"_id\"],\n",
    "        \"parentId\": \"\",\n",
    "        \"type\": dataset[\"_modelType\"],\n",
    "    })\n",
    "    # Begin folder search\n",
    "    for first in iterator:\n",
    "        # Reset the outputs\n",
    "        signals = []\n",
    "        rawdata_files = {param: [] for param in RAWDATA_FILES.values()}\n",
    "        # Iterate across the new folders\n",
    "        for folder in itertools.chain([first], itertools.islice(iterator, MAX_VIP_JOBS - 1)):\n",
    "            # New signal\n",
    "            signals.append(folder)\n",
    "            # Get the rawdata files\n",
    "            filenames = get_files(folder)\n",
    "            # Fill the parameter matrix\n",
    "            for param in rawdata_files:\n",
    "                rawdata_files[param].append(filenames[param] if param in filenames else \"\")\n",
    "        # Yield\n",
    "        yield signals, rawdata_files\n",
    "# ------------------------------------------------\n",
    "\n",
    "# Function to create a Girder folder\n",
    "def new_output_dir(parentId: str) -> str:\n",
    "    try: \n",
    "        return GIRDER_CLIENT.createFolder(\n",
    "            parentId=parentId, name=OUTPUT_FOLDER, metadata=OUTPUT_METADATA, description=OUTPUT_DESCRIPTION\n",
    "            )[\"_id\"]\n",
    "    except HttpError:\n",
    "        print(f\"\"\"(!) Output folder '{OUTPUT_FOLDER}' already exists in parent: '{parentId}' on Girder.\n",
    "            **Please restart the Notebook or delete the folder on Girder.***\"\"\")\n",
    "        raise\n",
    "# ------------------------------------------------\n",
    "\n",
    "# Generator to launch executions on VIP and return the Girder ID of the created folders\n",
    "def launch_executions() -> dict:\n",
    "    # Current running workflow \n",
    "    workflow_id = None\n",
    "    # Browse the dataset\n",
    "    print(\"Looking for the first acquisitions... \", end=\"\", flush=True)\n",
    "    for signals, rawdata_files in get_data():\n",
    "        print(\"Done.\")\n",
    "        # Wait for the previous execution\n",
    "        if workflow_id:\n",
    "            print(\"- Waiting for the current workflow to end (this can be monitored on the VIP website)... \", end=\"\", flush=True)\n",
    "            while vip.execution_info(workflow_id)[\"status\"] == \"Running\":\n",
    "                time.sleep(5)\n",
    "            print(\"Done.\")\n",
    "        print(\"\\nNew signals:\\n\\t\", \"\\n\\t\".join([str(sig[\"path\"].parent) for sig in signals]), sep=\"\")\n",
    "        # Create the output folders on Girder\n",
    "        print(\"- Creating the output folders on Girder... \", end=\"\", flush=True)\n",
    "        output_dirs = {\n",
    "            str(sig[\"path\"].parent): new_output_dir(sig[\"parentId\"]) for sig in signals\n",
    "        }\n",
    "        print(\"Done.\")\n",
    "        # Yield the list of folders ID\n",
    "        yield output_dirs\n",
    "        # Update the VIP inputs with user's parameters and the current outputs directories\n",
    "        rawdata_files.update(PARAMETERS)\n",
    "        rawdata_files[\"results-directory\"] = [GIRDER_PREFIX + output_dirs[sig] for sig in output_dirs]\n",
    "        # Launch a new worflow on VIP\n",
    "        print(\"- Launching a new workflow on VIP... \", end=\"\", flush=True)\n",
    "        workflow_id = vip.init_exec_without_resultsLocation(\n",
    "            pipeline = PIPELINE_ID,\n",
    "            name = OUTPUT_FOLDER,\n",
    "            inputValues = rawdata_files,\n",
    "        )\n",
    "        print(\"Done. Current workflow:\", workflow_id)\n",
    "        # Prepare the next iteration\n",
    "        print(\"- Looking for new signals to process... \", end=\"\", flush=True)\n",
    "    # End of the loop\n",
    "    print(\"No more signals to process.\")\n",
    "    # Wait for the previous execution\n",
    "    if workflow_id:\n",
    "        print(\"Waiting for the current workflow to end ... \", end=\"\", flush=True)\n",
    "        while vip.execution_info(workflow_id)[\"status\"] == \"Running\":\n",
    "            time.sleep(5)\n",
    "        print(\"Done.\")\n",
    "    # End display\n",
    "    print(\"END\")\n",
    "# ------------------------------------------------\n",
    "\n",
    "# Main procedure\n",
    "if __name__ == \"__main__\":\n",
    "    # Handshake with VIP\n",
    "    vip.setApiKey(VIP_KEY)\n",
    "    # Handshake with Girder\n",
    "    GIRDER_CLIENT.authenticate(apiKey=GIRDER_KEY)\n",
    "    # Launch executions on VIP\n",
    "    all_outputs = {} # output metadata\n",
    "    try: # Output metadata are recorded at each iteration to have them registered in case of error\n",
    "        for output_dirs in launch_executions(): all_outputs.update(output_dirs) \n",
    "    except: raise\n",
    "    finally: # Output metadata are saved in a Json file to handle them later automatically\n",
    "        OUTPUT_METADATA.update({\n",
    "            \"name\": OUTPUT_FOLDER,\n",
    "            \"outputs\": all_outputs\n",
    "        })\n",
    "        with open(OUTPUT_FOLDER + \".json\", 'w') as save_file:\n",
    "            json.dump(OUTPUT_METADATA, save_file, indent=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database Cleanup\n",
    "Run the following cell *only* to **erase previous results on Girder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the output folder to suppress under every signal\n",
    "output_name = \"\" # ex: \"Proc_23-07-25_17-03-59\"\n",
    "# Delete from the JSON file\n",
    "if output_name:\n",
    "    metadata_file = output_name + \".json\"\n",
    "    # Open the JSON file\n",
    "    with open(metadata_file, 'r') as save_file:\n",
    "        metadata = json.load(save_file)\n",
    "    # Browse outputs\n",
    "    print(\"Deleting the output folders on Girder...\")\n",
    "    for folderPath in metadata[\"outputs\"]:\n",
    "        print(folderPath, \": \", end=\"\") \n",
    "        # Delete the folder\n",
    "        try :\n",
    "            rep = GIRDER_CLIENT.delete(path=\"folder/\" + metadata[\"outputs\"][folderPath])\n",
    "            print(rep[\"message\"])\n",
    "        except HttpError as e:\n",
    "            if e.status == 400:\n",
    "                print(rep[\"message\"])\n",
    "            else: raise\n",
    "    print(\"Done.\")\n",
    "    os.remove(metadata_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
